{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook demonstrates how to use YOLOv3 (You Only Look Once) for real-time object detection using a laptop's webcam. The code captures video from the webcam, processes each frame using the YOLOv3 model, and displays the detected objects in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import the necessary libraries. We'll use OpenCV for video capture and image processing, and NumPy for handling arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the YOLOv3 model using the weights and configuration files. We also need to load the COCO class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv3 weights and configuration\n",
    "weights_path = 'yolov3.weights'\n",
    "config_path = 'yolov3.cfg'\n",
    "coco_names_path = 'coco.names'\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(weights_path, config_path)\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(coco_names_path, 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll capture video from the default webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  # 0 for the default webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters for image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 1/255.0  # Normalizes pixel values\n",
    "size = (416, 416)  # Resize the image to 416x416 as required by YOLO\n",
    "swap_rb = True  # Swap red and blue channels\n",
    "crop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the output layer names for YOLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = net.getLayerNames()  # Retrieves names of all layers in the model\n",
    "output_layers_indices = net.getUnconnectedOutLayers() # Gets indices of output layers\n",
    "output_layers = [layer_names[i - 1] for i in output_layers_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main real-time detection loop starts by processing each frame from the webcam and perform object detection. First, the code captures a frame from the webcam. The `ret` value is a boolean indicating whether the frame was successfully read. If `ret` is `False`, the loop breaks, meaning the video capture has ended or there was an issue with reading the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the dimensions of the captured frame. `height` and `width` are used to scale bounding boxes and text correctly on the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    height, width, _ = frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It then converts the image into a blob suitable for YOLO input. This involves resizing the image to 416x416 (as required by YOLO), normalizing pixel values, and swapping color channels if necessary. And then sets the prepared blob as input to the YOLO network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    blob = cv2.dnn.blobFromImage(frame, scalefactor=scale_factor, size=size, swapRB=swap_rb, crop=crop)\n",
    "    net.setInput(blob) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, performs a forward pass through the network and retrieves the outputs from the specified output layers. `outs` contains the raw detection data from YOLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    outs = net.forward(output_layers) \n",
    "\n",
    "    class_ids = [] # Lists to store the IDs of detected classes.\n",
    "    confidences = [] # Their confidence scores.\n",
    "    boxes = [] # Their bounding box coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints the shape of each output layer. YOLOv3 generates multiple outputs, and their shapes can vary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for out in outs:\n",
    "        print(f\"Output layer shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks if the output is a 2D array, as for YOLOv3, each detection is expected to have 85 values: 4 for bounding box coordinates (x_center, y_center, width, height), 1 for objectness score, and 80 for class scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if len(out.shape) == 2:\n",
    "            num_detections, num_values = out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts bounding box coordinates, objectness score, and class scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            if num_values == 85:\n",
    "                for det in out:\n",
    "                    x_center = det[0]\n",
    "                    y_center = det[1]\n",
    "                    w = det[2]\n",
    "                    h = det[3]\n",
    "                    objectness = det[4]\n",
    "                    scores = det[5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts normalized bounding box coordinates to pixel values based on the frame dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    center_x = int(x_center * width)\n",
    "                    center_y = int(y_center * height)\n",
    "                    w = int(w * width)\n",
    "                    h = int(h * height)\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters detections based on objectness and confidence scores. If the objectness and confidence exceed thresholds, the detection is considered valid and added to the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    if objectness > 0.5:\n",
    "                        class_id = np.argmax(scores)\n",
    "                        confidence = scores[class_id]\n",
    "                        if confidence > 0.5:  # Confidence threshold\n",
    "                            boxes.append([x, y, w, h])\n",
    "                            confidences.append(float(confidence))\n",
    "                            class_ids.append(class_id)\n",
    "            else:\n",
    "                print(f\"Unexpected number of values per detection: {num_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 3D outputs (i.e., multiple detections per layer), processes each detection similarly as the 2D outputs. Also handles cases where the output shape does not match the expected formats, printing an error message in both outputs cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        elif len(out.shape) == 3: \n",
    "            for detection in out:\n",
    "                if detection.shape[1] == 85:\n",
    "                    for det in detection:\n",
    "                        # Extract values from the detection array\n",
    "                        x_center = det[0]\n",
    "                        y_center = det[1]\n",
    "                        w = det[2]\n",
    "                        h = det[3]\n",
    "                        objectness = det[4]\n",
    "                        scores = det[5:]\n",
    "\n",
    "                        # Convert to image coordinates\n",
    "                        center_x = int(x_center * width)\n",
    "                        center_y = int(y_center * height)\n",
    "                        w = int(w * width)\n",
    "                        h = int(h * height)\n",
    "                        x = int(center_x - w / 2)\n",
    "                        y = int(center_y - h / 2)\n",
    "\n",
    "                        # Filter based on objectness and confidence\n",
    "                        if objectness > 0.5:\n",
    "                            class_id = np.argmax(scores)\n",
    "                            confidence = scores[class_id]\n",
    "                            if confidence > 0.5:  # Confidence threshold\n",
    "                                boxes.append([x, y, w, h])\n",
    "                                confidences.append(float(confidence))\n",
    "                                class_ids.append(class_id)\n",
    "                else:\n",
    "                    print(f\"Unexpected number of values per detection: {detection.shape[1]}\")\n",
    "        else:\n",
    "            print(f\"Unexpected output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies non-max suppression to eliminate redundant overlapping bounding boxes. Only the box with the highest confidence is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, score_threshold=0.5, nms_threshold=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each valid detection, draws a bounding box and label on the frame. The label includes the class name and confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if len(indices) > 0:\n",
    "        indices = indices.flatten()\n",
    "        for i in indices:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = f\"{classes[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows the current frame with detected objects in a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    cv2.imshow('YOLO Real-Time Detection', frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allows exiting the loop by pressing the 'q' key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the loop ends, release the webcam and close all the display window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
